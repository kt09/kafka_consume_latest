{
   "scenarioName": "Unload - consume a message from kafka - raw dump to file",
   "steps": [
      {
         "name": "unload_and_commit",
         "url": "kafka-topic:demo",
         "operation": "consume",
         "request": {
            "consumerLocalConfigs": {
               "commitSync": true,
               //"commitAsync": true, //Uncomment to test validation
               "showRecordsAsResponse": false,
               "MAX_NO_OF_RETRY_POLLS_OR_TIME_OUTS": 5
            }
         },
         "assertions": {
         }
      },
      {
         "name": "load_kafka",
         "url": "kafka-topic:demo",
         "operation": "produce",
         "request": {
            "key": "${RANDOM.NUMBER}",
            "value": "Hello World"
         },
         "assertions": {
            "status": "Ok"
         }
      },
      {
         "name": "dump_file_record_count",
         "url": "kafka-topic:demo",
         "operation": "consume",
         "request": {
            "consumerLocalConfigs": {
               "fileDumpTo": "target/temp/demo.txt",
               "fileDumpType": "RAW",
               //"commitSync":true, // not effective, bacaz it does a commit after consuming the records.
               // See step 'unload_and_commit', after which the reading starts fresh from the next offset.
               // Otherwise you get the earlier records form last committed offset.
               "showRecordsAsResponse": false,
               "MAX_NO_OF_RETRY_POLLS_OR_TIME_OUTS": 5
            }
         },
         "assertions": {
            "status": "Ok",
            "recordCount": 1
         }
      }
   ]
}
